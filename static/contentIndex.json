{"Graph-theory/Bottleneck-Design":{"slug":"Graph-theory/Bottleneck-Design","filePath":"Graph-theory/Bottleneck Design.md","title":"Bottleneck Design","links":["tags/bottleneckDesign"],"tags":["bottleneckDesign"],"content":"bottleneckDesign\nInput Layer (1 feature per node)\n   │\n   ▼\nSpatio-Temporal Block 1\n  ┌─────────────────────────────────┐\n  │ Temporal Conv (Kernel Size: Kt) │\n  │ Graph Convolution (Spatial)     │\n  │ Activation (GLU or GTU)         │\n  └─────────────────────────────────┘\n   │\n   ▼\nSpatio-Temporal Block 2\n  ┌─────────────────────────────────┐\n  │ Temporal Conv (Kernel Size: Kt) │\n  │ Graph Convolution (Spatial)     │\n  │ Activation (GLU or GTU)         │\n  └─────────────────────────────────┘\n   │\n   ▼\nFully Connected Layer (128 neurons)\n   │\n   ▼\nFully Connected Layer (128 neurons)\n   │\n   ▼\nOutput Layer (1 feature per node)\n\nThe architecture of the Spatio-Temporal Graph Convolutional Network (STGCN) follows a structured approach to extract features from spatio-temporal data. The blocks variable defines the layer configuration:\nblocks = [[1], [64, 16, 64], [64, 16, 64], [128, 128], [1]]\nThis breakdown explains the function of each layer in the network:\n1. Input Layer\n\n[1] at the beginning represents the input layer, where each node has a single feature (e.g., sensor readings, time-series data).\n\n2. Spatio-Temporal Convolutional Blocks (STConvBlocks)\nThe network has two STConvBlocks, each defined as:\n[64, 16, 64]\n\n64 channels: Initial feature representation.\n16 channels: Bottleneck layer to reduce computation while maintaining expressive power.\n64 channels: Expands back to retain necessary information.\n\nThese blocks allow the model to extract meaningful patterns from spatial and temporal data efficiently.\n3. Fully Connected (Dense) Layers\n\nThe two [128, 128] layers come after the STConvBlocks and serve as fully connected layers.\nTheir purpose:\n\nProcess and refine extracted features before generating the final prediction.\nAllow additional transformations when Ko &gt; 0, ensuring the network adapts to remaining temporal steps.\n\n\n\n4. Output Layer\n\nThe final [1] ensures the model outputs one predicted value per node.\nThis is the target prediction, which could represent values like future traffic speed, sensor readings, or other temporal data.\n\nLayer Breakdown Summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLayer TypeConfigurationInput Layer[1] (Single input feature per node)STConvBlock 1[64, 16, 64] (Spatio-Temporal feature extraction)STConvBlock 2[64, 16, 64] (Another STConvBlock for deeper features)Dense Layer 1[128] (Processes extracted features)Dense Layer 2[128] (Further transformation before output)Output Layer[1] (Final prediction per node)\nWhy Two Fully Connected Layers?\n\nWhen Ko &gt; 0, additional transformations are necessary to refine feature representations before prediction.\nThese layers allow the model to learn complex relationships beyond the convolutional layers.\n\nKey Takeaways\n\nThe architecture is designed for efficient spatio-temporal feature extraction.\nUses a bottleneck strategy (64 → 16 → 64) to optimize computation.\nFully connected layers (128, 128) enhance feature transformation before prediction.\nThe final layer ensures one output per node, making it suitable for time-series forecasting in graphs.\n"},"Graph-theory/GCN-Kernel":{"slug":"Graph-theory/GCN-Kernel","filePath":"Graph-theory/GCN Kernel.md","title":"GCN Kernel","links":[],"tags":[],"content":"In Graph Convolutional Networks (GCNs):\n• The kernel is not a fixed matrix sliding across nodes.\n• Instead, it is a function that propagates information across the graph using the graph Laplacian.\n• This kernel defines how a node aggregates information from its neighbors.\nThe standard spectral graph convolution is defined as:\ng_{\\theta} \\star x = U g_{\\theta}(\\Lambda) U^T x\nwhere:\n• U and \\Lambda are eigenvectors and eigenvalues of the graph Laplacian.\n• g_{\\theta}(\\Lambda) is a function that filters the graph signals in the spectral domain.\n• x is the node feature matrix.\nThis requires an eigenvalue decomposition, which is slow!\nInstead, we use a Chebyshev polynomial approximation to avoid direct computation of eigenvalues."},"Graph-theory/Graph-Shift-Operators":{"slug":"Graph-theory/Graph-Shift-Operators","filePath":"Graph-theory/Graph Shift Operators.md","title":"Graph Shift Operators","links":["Graph-theory/adjacency-matrix","Laplacian-matrix","Graph-theory/Laplacian"],"tags":[],"content":"1. Mathematical Foundations of GSOs\nDefinition: A Graph Shift Operator (GSO) is a linear operator on graph signals that respects the graph’s sparsity structure. Formally, for a graph G=(V,E) with adjacency matrix A, a matrix S \\in \\mathbb{R}^{n\\times n} is a GSO if S_{ij} = 0 whenever there is no edge from node j to node i (i.e. if i \\neq j and (i,j) \\notin E then S_{ij}=0 (Segarra et al., 2015) . In other words, S can have nonzero entries only along existing edges (and optionally on the diagonal for self-loops). Common examples of GSOs include the adjacency matrix, the Laplacian matrix, and their normalized variants (Dasoulas et al., 2021).These matrices all “shift” a graph signal by mixing information from a node’s neighbors. Multiplying a signal vector x by the adjacency GSO A produces a new signal y = A x where each component y_i = \\sum_{j:(i,j)\\in E} A_{ij} x_j is a weighted sum of node i’s neighbors’ values (Segarra et al., 2015) – analogous to shifting a time-series signal.\nSpectral Graph Theory Context: In spectral graph theory (and graph signal processing), the choice of GSO defines a graph Fourier basis1. If S is diagonalizable as S = V \\Lambda V^{-1} (e.g. for a symmetric GSO, V can be chosen orthonormal), its eigenvectors V serve as Fourier basis and eigenvalues \\Lambda represent “frequencies” (Segarra et al., 2015). For instance, using the graph Laplacian L (which is symmetric for an undirected graph) as the GSO, the eigen-decomposition L = U \\Lambda U^T yields the graph Fourier transform U^T x. Low eigenvalues correspond to smooth varying signals and high eigenvalues to rapid changes on the graph. Using the adjacency matrix as S similarly yields a spectrum, though the Laplacian is more commonly associated with spectral graph theory due to its nice properties (e.g. positive semi-definiteness and a notion of frequency ranging from 0 upward). Notably, the normalized Laplacian is directly related to the normalized adjacency: for an undirected graph, the symmetric normalized Laplacian is L_{\\text{sym}} = I_n - D^{-1/2} A D^{-1/2} (Lutzeyer, 2022). Thus, the normalized adjacency D^{-1/2} A D^{-1/2} can be seen as I_n - L_{\\text{sym}}, meaning it shares an eigenbasis with L_{\\text{sym}} but with eigenvalues flipped into the range [-1,1]. This highlights the close relationship between GSOs and Laplacians – they are often interchangeable choices for defining graph frequencies or shifts.\nGraph Filtering and Convolutions: GSOs generalize the concept of signal shifting (and hence convolution) from regular domains (like time or images) to irregular graph domains. In classical DSP, a linear time-invariant filter can be expressed as a polynomial in the shift operator (delay). Analogously, a graph filter is defined as a polynomial in a GSO (Segarra et al., 2015). For example, a graph filter H can be written as H = \\sum_{k=0}^{L} h_k S^k, which means H takes a weighted combination of the signal shifted 0 hops (itself), 1 hop (neighbors via S), 2 hops (S^2 neighbors-of-neighbors), and so on (Segarra et al., 2015).\n\nH is the graph filter, which transforms the signal by aggregating information from its neighbors.\n\\sum_{k=0}^{L} represents summation over different shift levels, from k = 0 (the node itself) to k = L (farther neighbors).\nh_k are filter coefficients that control how much weight is given to each shift.\nS is the Graph Shift Operator (GSO), such as the adjacency or Laplacian matrix.\nS^k represents applying S k times, meaning it shifts the signal across the graph up to k hops away - S^0 = I (which represents the signal itself), S^1 = S (first shift, i.e., direct neighbors), S^k (shifting the signal k hops in the graph)\nL is the filter order, determining how far the filter extends in the graph.\n\nThis polynomial form ensures the filtering is shift-invariant on the graph: applying the filter then one shift is equivalent to one shift then the filter . Intuitively, such filters generalize convolution to graphs – rather than sliding a window over a sequence, we aggregate information from neighbors within k-hop neighborhoods. The concept extends to spectral filtering: if S is diagonalized by V \\Lambda V^{-1}, the polynomial H(S) will be diagonalized by the same V, meaning it acts by scaling the graph’s Fourier modes (eigenvectors of S) by h(\\lambda) (the polynomial applied to eigenvalues) (Segarra et al., 2015).\n\n2. Types of Normalization in GSOs\nGraph Neural Networks typically use normalized GSOs to improve training stability and performance. Different normalization strategies can be applied to the adjacency matrix (or Laplacian) when constructing the GSO:\nSymmetric normalization: Using the form \\tilde{A} = D^{-1/2} A D^{-1/2}. Here D is the diagonal degree matrix (D_{ii}=\\sum_j A_{ij}). This normalization treats an edge (i,j) with weight A_{ij} symmetrically from both endpoint i and j’s perspective, dividing by the square root of each node’s degree. Symmetric normalization is used in the popular Graph Convolutional Network of Kipf &amp; Welling (Lutzeyer, 2022). It ensures the resulting operator is symmetric (for undirected graphs) and has eigenvalues bounded in [-1,1], which helps convergence. Intuitively, it normalizes neighbor contributions by both the source and target node degrees, preventing high-degree nodes from overwhelming their neighbors. The GCN update can be written as H^{(l+1)} = \\sigma(D^{-1/2} A D^{-1/2} \\, H^{(l)} W^{(l)}), where:\n\nH^{(l+1)}: The feature matrix at layer l+1, with dimensions n \\times d_{l+1}, where n is the number of nodes, and d_{l+1} is the number of output features per node at layer l+1.\n\\sigma: An activation function applied element-wise, such as ReLU (Rectified Linear Unit), introducing non-linearity into the model.\nD: The degree matrix, a diagonal matrix where each diagonal element D_{ii} represents the degree (number of connections) of node i. Mathematically, D_{ii} = \\sum_j A_{ij}.\nA: The adjacency matrix of the graph, an n \\times n matrix where A_{ij} indicates the presence (and possibly weight) of an edge between nodes i and j.\nD^{-\\frac{1}{2}} A D^{-\\frac{1}{2}}: The normalized adjacency matrix, which ensures that the feature aggregation accounts for the degrees of nodes, promoting numerical stability and improving model performance.\nH^{(l)}: The feature matrix at layer l, with dimensions n \\times d_l, where d_l is the number of features per node at layer l.\nW^{(l)}: The trainable weight matrix at layer l, with dimensions d_l \\times d_{l+1}, used to transform the feature space from d_l to d_{l+1},\nmeaning each layer multiplies by the symmetrically normalized adjacency (Yu et al., 2018). This choice has been shown to stabilize training – by keeping the scale of features consistent – and improve accuracy on tasks like node classification. In fact, the normalization brings the operator’s largest eigenvalue to 1, avoiding exploding values when applying multiple graph convolutions in deep networks (Yu et al., 2018).\n\nSelf-loop addition and renormalization: A technique introduced to improve stability is to add self-loops to the adjacency matrix and then normalize. Kipf &amp; Welling (GCN) (Kipf &amp; Welling, 2016) proposed using \\tilde{A} = A + I_n (where I_n is the identity matrix, adding a self-connection on every node) and \\tilde{D}_{ii} = \\sum_j \\tilde{A}_{ij}, then applying symmetric normalization \\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2} (Yu et al., 2018). This renormalization trick has two effects: (1) Adding I_n ensures that when a node updates its features, it retains a portion of its own previous features (a self-pass), which helps preserve information and combat over-smoothing. (2) The degree for normalization increases by 1 for each node, which moderates the impact of highly connected nodes. Kipf &amp; Welling (Kipf &amp; Welling, 2016) found that with a first-order approximation of the spectral graph convolution, setting \\theta_0 = -\\theta_1 and renormalizing A+I allowed them to collapse two parameters into one and “stabilize numerical performance” during training . In equation form, their convolution became H^{(l+1)} = \\sigma(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2} H^{(l)} W^{(l)}) = \\sigma((I_n + D^{-1/2}A D^{-1/2}) H^{(l)} W^{(l)}) (Yu et al., 2018). This renormalized GSO has been widely adopted due to its improved training stability – essentially keeping the spectrum of the operator in a controlled range and ensuring the model doesn’t diverge. It’s worth noting that other normalization variations exist (for example, some works use column-normalization or other heuristics), but symmetric and random-walk are by far the most common in GNNs, often with self-loops added in modern architectures.\n\n3. Comparison with Alternative Methods\nGSOs vs Graph Laplacians: The graph Laplacian (in either unnormalized or normalized form) is closely related to GSOs and is often used in spectral methods. In fact, as noted above, choosing the normalized Laplacian L_{\\text{sym}} as the GSO is equivalent to using I - D^{-1/2} A D^{-1/2}. Many spectral clustering and graph signal processing techniques traditionally use L or L_{\\text{sym}} because their eigenvalues/eigenvectors have nice interpretations (e.g. minimizing Laplacian quadratic forms leads to community structure) (Lutzeyer, 2022). However, in the context of graph neural networks, using the Laplacian or the adjacency as the shift operator ultimately leads to a similar notion of graph convolution – one can be converted to the other. For example, a first-order polynomial in the normalized Laplacian, I - L_{\\text{sym}}, is exactly the normalized adjacency. Higher-order polynomials in L expand to mixed terms of I and powers of A. The key difference is interpretability: an adjacency-based propagation (H^{(l+1)} = A H^{(l)} with appropriate normalization) is interpreted as each node receiving messages from neighbors (a direct message-passing view), whereas a Laplacian-based propagation (H^{(l+1)} = H^{(l)} - D^{-1}A H^{(l)}, for instance) can be seen as each node adjusting its state by subtracting a portion of the neighbor-averaged state (a diffusion or smoothing view). In practice, most modern GNNs use the adjacency (with normalization) as the GSO in the message-passing formulation (Dasoulas et al., 2021b). The choice between Laplacian or adjacency is sometimes a matter of convenience: spectral formulations naturally yield Laplacian polynomials, while spatial formulations explicitly use adjacency. Both are valid GSOs and are related by L = D - A.\nSpectral Filtering vs Spatial (GSO-based) Convolution: Early graph neural networks (Bruna et al., 2014) were formulated in the spectral domain, performing convolutions by multiplying the Fourier coefficients of signals by a filter function g(\\lambda) in the graph frequency domain. While theoretically elegant, this approach required computing the eigendecomposition of the Laplacian (O(n^3) in general) and the filters were not localized (a spectral filter is a global operation) (Yu et al., 2018). To address these issues, research introduced polynomial approximations of spectral filters. Hammond et al. (Hammond et al., 2011) showed that any smooth filter can be approximated by a Chebyshev polynomial of the Laplacian eigenvalues . This led to ChebNet (Defferrard et al., 2016), which uses Chebyshev polynomials of the Laplacian as the convolution kernels. A K-th order Chebyshev polynomial in L corresponds to using up to K-hop neighbors (via L^k or equivalently A^k powers) in the filter, making the operation localized. Importantly, this made the convolution linear in the number of edges |E| (O(K|E|) complexity) instead of requiring costly spectral transforms . Kipf &amp; Welling (Kipf &amp; Welling, 2016) further simplified this with a first-order polynomial (linear function) of the normalized Laplacian, which as they showed is equivalent to using the normalized adjacency with a self-loop (their renormalization trick) . Thus, the spatial approach to graph convolution – using the GSO (like adjacency) directly to mix neighbor information – has a major efficiency advantage and naturally aligns with the message-passing interpretation of GNNs. Thus, spectral graph convolutions are polynomial filters of a GSO (typically the Laplacian), so they can be implemented as repeated multiplications by that GSO in the node domain. Modern GNN layers essentially perform H^{(l+1)} = \\sigma(S H^{(l)} W) (for some choice of GSO S), rather than explicitly doing an eigenvector transform for convolution. This is why GSOs are central: they unify spectral and spatial methods by serving as the base operator for graph filtering.\nAdvantages of GSOs for Spatio-Temporal GNNs: In spatio-temporal applications, data comes as time sequences of graph signals (e.g. number of vehicles on a road network over time). GSOs are crucial in these models to capture spatial dependencies while other mechanisms (like RNNs or temporal convolutions) capture temporal dependencies. A clear advantage of using a GSO (graph-based convolution) is that it can model complex spatial relationships that a standard temporal model would miss. For example, in traffic forecasting, roads form a graph and congestion propagates along connected roads. The Spatio-Temporal GCN by Yu et al. (Yu et al., 2018) uses graph convolution (based on Chebyshev polynomials of theLaplacian, later simplified to the GCN approach) to capture the spatial influence of traffic flow, combined with 1D convolutions across time for temporal patterns. This method can utilize information from nearby roads (neighbors in the graph) up to K hops away if K graph convolution layers are stacked. In fact, stacking multiple graph conv layers increases the receptive field: with K sequential layers (each using 1-hop neighbors), a node can receive information from K-hop neighbors . This stacking is an alternative to using a single high-order polynomial filter; it was found to be parameter-efficient and achieved similar effect as a wide K-hop filter (Yu et al., 2018).\n\n4. Practical Applications and Use Cases\nChoosing GSOs for Different Tasks: The optimal choice of GSO (adjacency vs Laplacian, symmetric vs random-walk normalization, etc.) can be task-dependent . Below are some common scenarios and recommended practices:\nTraffic and diffusion processes: For transportation networks, random-walk normalized adjacency is often appropriate. Traffic flow has a directional nature (e.g. congestion moves along directed roads), so using D^{-1}A (and its transpose for opposite direction) captures the asymmetric diffusion well. Models like DCRNN explicitly use this to let information diffuse through the graph akin to a random walk (Li et al., 2018). If the road network is treated as undirected or one is interested in general connectivity, adding symmetric normalization (or using the Laplacian’s Chebyshev polynomials as in ST-GCN) also works, but one must be careful with directed edges. In practice,\n\ndirected graphs → prefer D_{\\text{out}}^{-1}A;\nundirected graphs → symmetric D^{-1/2} A D^{-1/2}\n\nis a safe choice. Traffic forecasting experiments have shown that graph-based models significantly outperform models that don’t use the graph. For example, DCRNN (with a diffusion GSO) and ST-GCN (with Chebyshev/GSO) both substantially outperformed LSTM or ARIMA baselines on real traffic datasets (Li et al., 2018), confirming the value of the GSO.\nTrade-offs (Accuracy, Efficiency, Stability): Selecting a GSO and its normalization involves balancing several factors:\n• Accuracy: The right GSO can significantly boost accuracy by injecting domain knowledge of connectivity. For example, including a self-loop (identity) often improves accuracy by preserving each node’s own information alongside neighbor info, which many studies found beneficial (Dasoulas et al., 2021b), (Lutzeyer, 2022). On the other hand, using a poorly chosen GSO (e.g., ignoring directionality in a directed graph, or using an overly smoothed Laplacian) might underfit important patterns. There is no one-size-fits-all: recent research suggests the optimal GSO representation is data- and task-dependent (Dasoulas et al., 2021b)– some tasks benefit from Laplacian-based operators, others from raw adjacency, etc. Thus, accuracy is maximized when the GSO aligns well with the graph phenomena of interest (e.g. diffusion, communities, etc.).\n• Efficiency: Simpler normalizations (or no normalization) can be slightly more efficient per iteration, but the difference is usually minor since all are O(|E|) operations. The bigger efficiency considerations are in model design: spectral methods that require computing eigenvectors are costly, whereas polynomial filters using a GSO are fast. For instance, Chebyshev polynomials brought complexity down to O(K|E|) from O(n^2) (Yu et al., 2018), enabling scalability to large graphs. In practice, using the normalized adjacency vs unnormalized doesn’t change big-O complexity, but normalized may converge in fewer training epochs (a form of efficiency) because it smooths the learning process. Also, certain normalizations can be precomputed (e.g. precompute D^{-1/2}A D^{-1/2} once). One edge-case: if a graph is very dense, computing D^{-1/2} may be heavy, but such graphs are rare in many GNN applications (graphs are often sparse).\n• Stability and Training Dynamics: Proper normalization generally improves the numerical stability of GNN training (Yu et al., 2018). Without normalization, repeated multiplication by A can cause the scale of representations to blow up (especially if the graph has high-degree nodes or many layers, the values can grow exponentially). Symmetric normalization guarantees the operator’s norm is at most 1 for undirected graphs, acting like a contraction that keeps feature magnitudes in check. Random-walk normalization ensures each node’s update is a weighted average, preventing divergence. These normalizations also mitigate the issue of distribution shift between nodes of different degree – making the network’s job easier when learning a single set of weights W^{(l)} to apply at all nodes. However, there is a known side-effect if one applies many graph convolutions sequentially: over-smoothing. If the GSO (especially a normalized one) is applied repeatedly (many layers), eventually all node representations in each connected component tend to converge (because the operator has an eigenvalue 1 corresponding to the constant vector, and other modes may decay) – meaning nodes become indistinguishable. This is why very deep GCNs can lose accuracy. Techniques like adding self-loops (which we do) and residual connections or normalization can delay oversmoothing, but it remains a trade-off: using a GSO enables graph mixing but too much of it washes out differences. In practice, a few GNN layers (e.g. 2–4) are used, or methods to re-inject original features are employed (as in APPNP or Jumping Knowledge networks).\nEmpirical Comparisons: Studies have compared different GSOs and normalizations on benchmark tasks:\n• Kipf &amp; Welling (2017) showed that a first-order (normalized adjacency) GCN achieved comparable or better accuracy on citation network classification than higher-order spectral models, while being simpler . Their use of symmetric normalization + self-loop was crucial to achieve 81% accuracy on Cora, whereas using an unnormalized adjacency caused training to diverge or perform poorly (they note the necessity of the normalization trick for stability) .\n• The GraphSAGE paper observed that using a mean aggregator (which corresponds to D^{-1}A) yielded similar performance to a GCN-style aggregator on inductive node classification, and found no significant difference between mean and more complex pooling on certain datasets . This suggests that for those tasks, as long as some reasonable normalization is in place, the exact type (mean vs symmetric) was not critical. But this might not hold universally.\n• Recent research by Dell’Amico et al. (2020) and others found that in graph classification tasks, including the Laplacian eigenvectors or using Laplacian-based kernels can improve results in certain settings, indicating that the adjacency GSO alone didn’t capture all structural information . This has motivated adaptive methods (like learning a parametrized GSO or combining multiple GSOs). For example, Klicpera et al. (2019) combined the adjacency-based propagation with personalized PageRank (which effectively adds long-range random-walk information) to boost performance – essentially using an extended GSO that accounts for higher-hop connections with diminishing weights.\n• In spatio-temporal tasks, experiments consistently show that models using graph GSOs outperform those that treat the data as a collection of independent series. DCRNN (using random-walk GSO) and ST-GCN (using Chebyshev/GSO) report significantly lower error than non-graph baselines on traffic flow datasets . Similarly, in recommender systems, GNN-based approaches (PinSage, etc.) beat matrix factorization baselines by leveraging the graph of interactions.\nOverall, Graph Shift Operators provide a principled and powerful way to inject graph structure into neural networks. By understanding the theoretical foundation (GSOs as generalized shift operators and filter bases) and the effects of normalization, practitioners can choose the appropriate GSO for their application. The advantages of GSOs – localized filtering, ability to handle arbitrary graph topology, and improved model performance on structured data – make them superior to naive approaches that ignore graph connectivity. Ongoing research continues to refine GSO choices (even learn them) to further improve GNN accuracy and stability .\n\nFootnotes:\n\n  Bruna, J., Zaremba, W., Szlam, A., &amp; LeCun, Y. (2014). Spectral Networks and Locally Connected Networks on Graphs. International Conference on Learning Representations (ICLR). arxiv.org/abs/1312.6203\n  Dasoulas, G., Lutzeyer, J. F., &amp; Vazirgiannis, M. (2021a). Learning Parametrised Graph Shift Operators. International Conference on Learning Representations (ICLR). openreview.net/pdf\n  Dasoulas, G., Lutzeyer, J. F., &amp; Vazirgiannis, M. (2021b). Learning Parametrised Graph Shift Operators. International Conference on Learning Representations. openreview.net/pdf\n  Defferrard, M., Bresson, X., &amp; Vandergheynst, P. (2016). Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. Advances in Neural Information Processing Systems, 29, 3844–3852. arxiv.org/abs/1606.09375\n  Hammond, D. K., Vandergheynst, P., &amp; Gribonval, R. (2011). Wavelets on Graphs via Spectral Graph Theory. Applied and Computational Harmonic Analysis, 30(2), 129–150. doi.org/10.1016/j.acha.2010.04.005\n  Kipf, T. N., &amp; Welling, M. (2016). Semi-Supervised Classification with Graph Convolutional Networks. arXiv Preprint arXiv:1609.02907.\n  Li, Y., Yu, R., Shahabi, C., &amp; Liu, Y. (2018). Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting. International Conference on Learning Representations (ICLR). openreview.net/forum\n  Lutzeyer, J. (2022). Graph Shift Operators and Their Relevance to Graph Neural Networks. johanneslutzeyer.com/doc/GraphShiftOperatorsAndTheirRelevanceToGraphNeuralNetworks_handout.pdf\n  Segarra, S., Mateos, G., Marques, A. G., &amp; Ribeiro, A. (2015). Blind Identification of Graph Filters with Sparse Inputs. 2015 IEEE 6th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 237–240. doi.org/10.1109/CAMSAP.2015.7383833\n  Yu, B., Yin, H., &amp; Zhu, Z. (2018). Spatio-temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting. Proceedings of the 27th International Joint Conference on Artificial Intelligence (IJCAI).\nFootnotes\n\n\nThe Graph Fourier Basis consists of the eigenvectors of a Graph Shift Operator (GSO), such as the graph Laplacian or adjacency matrix. These eigenvectors serve as the equivalent of sinusoidal functions in the classical Fourier transform, allowing graph signals to be decomposed into frequency components. Lower eigenvalues correspond to smooth variations across the graph, while higher eigenvalues represent rapid changes. ↩\n\n\n"},"Graph-theory/Laplacian":{"slug":"Graph-theory/Laplacian","filePath":"Graph-theory/Laplacian.md","title":"Laplacian","links":[],"tags":[],"content":""},"Graph-theory/Vertices":{"slug":"Graph-theory/Vertices","filePath":"Graph-theory/Vertices.md","title":"Vertices","links":["Graph-theory/adjacency-matrix"],"tags":[],"content":"Vertices in Graphs\nDefinition\nA vertex (or node) in a graph is a fundamental unit that represents an entity within the network. Formally, a graph is defined as:\nG = (V, E)\nwhere:\n\nV is the set of vertices (nodes),\nE is the set of edges (connections between vertices).\n\nEach vertex v \\in V can have attributes, such as labels, features, or temporal data, depending on the application.\nMathematical Representation\nVertices are indexed using natural numbers v_i for i \\in \\{1, ..., |V|\\}. The properties of vertices are commonly stored in:\n\nAdjacency matrices A where A_{ij} represents the connection weight between vertex v_i and v_j.\nFeature matrices X where each row corresponds to the features of a vertex.\n\nVertex Degree\nThe degree of a vertex is the number of edges connected to it. There are two types of degrees in a directed graph:\n\nIn-degree:d_{in}(v) = \\sum_{u \\in V} A_{uv}\nOut-degree:d_{out}(v) = \\sum_{w \\in V} A_{vw}\n\nIn an undirected graph, the degreed(v)is simply:\n[ d(v) = \\sum_{u \\in V} A_{uv} ]\nGraph Signal Representation\nIn Graph Neural Networks (GNNs), each vertex holds a graph signal that is propagated across edges. Ifxis the feature vector of a node, then applying the adjacency matrix yields:\n[ x’ = Ax ]\nwherex&#039;is the updated signal after incorporating neighboring information.\nApplications in Freight Modeling\nFor transportation networks, vertices represent road intersections or zones. Temporal features, such as freight flow from MASS-GT, are typically assigned to edges instead of vertices, capturing directionality in the network.\nSummary\nVertices play a crucial role in graph-based modeling, influencing how information propagates in networks. Understanding their properties helps define robust representations in Graph Neural Networks, particularly in applications like transportation modeling where edge-associated attributes (e.g., freight flow) must be carefully considered."},"Graph-theory/adjacency-matrix":{"slug":"Graph-theory/adjacency-matrix","filePath":"Graph-theory/adjacency matrix.md","title":"adjacency matrix","links":[],"tags":[],"content":""},"STGCN_hazdzz/model/model":{"slug":"STGCN_hazdzz/model/model","filePath":"STGCN_hazdzz/model/model.md","title":"model","links":[],"tags":[],"content":""},"STGCN_hazdzz/script/dataloader":{"slug":"STGCN_hazdzz/script/dataloader","filePath":"STGCN_hazdzz/script/dataloader.md","title":"dataloader","links":["graph-structure","Vertices","Graph-theory/adjacency-matrix"],"tags":[],"content":"load_adj()\nThe load_adj() function is responsible for loading the [[adjacency matrix]] of the graph structure from a stored .npz file. This adjacency matrix represents the connectivity between nodes and is a crucial input for the Spatio-Temporal Graph Convolutional Network.\n1. Loading the Adjacency Matrix\n    dataset_path = &#039;./data&#039;\n    dataset_path = os.path.join(dataset_path, dataset_name)\n    adj = sp.load_npz(os.path.join(dataset_path, &#039;adj.npz&#039;))\n\nThe adjacency matrix file (adj.npz) is stored inside the dataset folder.\nsp.load_npz() loads the adjacency matrix, which is stored in Compressed Sparse Row (CSR) format.\nThe file is retrieved from ./data/{dataset_name}/adj.npz, allowing flexibility for different datasets.\n\n2. Converting the Adjacency Matrix to CSC Format\n    adj = adj.tocsc()\n\nConverts the adjacency matrix from CSR (Compressed Sparse Row) format to CSC (Compressed Sparse Column) format. CSC format is often faster for column-based access, making it useful for certain types of graph computations. It improves memory efficiency when performing matrix-vector multiplications, which are common in graph neural networks.\n\n3. Defining the Number of Vertices\n    n_vertex = &lt;user-defined&gt;\n\nThe function returns n_vertex, which represents the number of nodes in the graph.\nThis should be manually specified for each dataset to ensure proper alignment with the adjacency matrix.\nWhen integrating a custom dataset, replace the predefined values with the correct number of nodes.\n\n4. Returning the adjacency matrix and Node Count\n    return adj, n_vertex\n\nThe function outputs:\n\nadj → The adjacency matrix in CSC sparse format.\nn_vertex → The total number of nodes in the dataset.\n\n\n"},"STGCN_hazdzz/script/script":{"slug":"STGCN_hazdzz/script/script","filePath":"STGCN_hazdzz/script/script.md","title":"script","links":[],"tags":[],"content":""},"STGCN_hazdzz/script/utility":{"slug":"STGCN_hazdzz/script/utility","filePath":"STGCN_hazdzz/script/utility.md","title":"utility","links":[],"tags":[],"content":"calc_gso()\n1. Overview\nThe function calc_gso computes the Graph Shift Operator (GSO) from a directed adjacency matrix. The GSO determines how node features propagate in a Graph Neural Network (GNN).\nIn the context of freight flow modeling with ST-SimNet, special design choices were made:\n\nPreserving directionality: No forced symmetrization.\nUsing Random Walk Normalization: Ensures flows are properly scaled.\nNo self-loops: Avoids artificial feedback loops in the freight flow model.\n\n\n2. Processing the Adjacency Matrix\nn_vertex = dir_adj.shape[0]\n \nif sp.issparse(dir_adj) == False:\n    dir_adj = sp.csc_matrix(dir_adj)\nelif dir_adj.format != &#039;csc&#039;:\n    dir_adj = dir_adj.tocsc()\n\nEnsures dir_adj (directed adjacency matrix) is stored in compressed sparse column (CSC) format, improving computational efficiency.\n\n\n3. Avoiding Symmetrization\nMany GNNs symmetrize the adjacency matrix:\nA_{\\text{sym}} = \\frac{1}{2} (A + A^T)\nHowever, this is removed in ST-SimNet to retain directional freight flow dependencies:\n# adj = dir_adj + dir_adj.T.multiply(dir_adj.T &gt; dir_adj) - dir_adj.multiply(dir_adj.T &gt; dir_adj)\n# adj = 0.5 * (dir_adj + dir_adj.transpose())\nFreight moves directionally, so this operation is skipped.\n\n4. Avoiding Self-Loops in Flow Prediction\nSome GNN models add self-loops to improve stability:\n\\tilde{A} = A + I\nHowever, in freight flow modeling, self-loops do not make sense (a location does not transport goods to itself), so we avoid it:\nif gso_type in [&#039;sym_renorm_adj&#039;, &#039;rw_renorm_adj&#039;, &#039;sym_renorm_lap&#039;, &#039;rw_renorm_lap&#039;]:\n    adj = adj + id  # A + I (adding self-connections) - not desired in ST-SimNet\n\n5. Symmetric Normalization (Not Used for Freight)\nif gso_type in [&#039;sym_norm_adj&#039;, &#039;sym_renorm_adj&#039;, &#039;sym_norm_lap&#039;, &#039;sym_renorm_lap&#039;]:\n    row_sum = adj.sum(axis=1).A1\n    row_sum_inv_sqrt = np.power(row_sum, -0.5)\n    row_sum_inv_sqrt[np.isinf(row_sum_inv_sqrt)] = 0.\n    deg_inv_sqrt = sp.diags(row_sum_inv_sqrt, format=&#039;csc&#039;)\n \n    sym_norm_adj = deg_inv_sqrt.dot(adj).dot(deg_inv_sqrt)\nThis computes the symmetric normalized adjacency matrix:\nA_{\\text{sym}} = D^{-0.5} A D^{-0.5}\nBut this method is NOT used in ST-SimNet because it destroys directionality.\n\n6. Random Walk Normalization for Freight Flow\nFor freight flow modeling, we apply random walk normalization, ensuring:\nA_{\\text{rw}} = D_{\\text{out}}^{-1} A\nwhere  D_{\\text{out}} is the out-degree matrix:\nD_{\\text{out}}(i,i) = \\sum_j A_{ij}\nif gso_type in [&#039;rw_norm_adj&#039;, &#039;rw_renorm_adj&#039;, &#039;rw_norm_lap&#039;, &#039;rw_renorm_lap&#039;]:\n    row_sum = np.sum(adj, axis=1).A1\n    row_sum_inv = np.power(row_sum, -1)\n    row_sum_inv[np.isinf(row_sum_inv)] = 0.\n    deg_inv = np.diag(row_sum_inv)\n \n    rw_norm_adj = deg_inv.dot(adj)  # D_out^-1 * A\nEnsures each location’s outgoing flows sum to 1\nPreserves directionality of freight movement\nIf a random walk Laplacian is needed:\nif gso_type in [&#039;rw_norm_lap&#039;, &#039;rw_renorm_lap&#039;]:\n    rw_norm_lap = id - rw_norm_adj\n    gso = rw_norm_lap\nelse:\n    gso = rw_norm_adj\nThis computes:\nL_{\\text{rw}} = I - D_{\\text{out}}^{-1} A\nwhich is useful for diffusion-based models.\n\n7. Out-Degree Normalization (Best for Freight)\nFor freight flow, the best choice is to normalize by out-degree:\nif gso_type in [&#039;rw_norm_adj_out&#039;, &#039;rw_norm_lap_out&#039;]:\n    row_sum = np.array(adj.sum(axis=1)).flatten()  # Out-degree\n    row_sum_inv = np.power(row_sum, -1)\n    row_sum_inv[np.isinf(row_sum_inv)] = 0.\n    deg_inv = sp.diags(row_sum_inv, format=&#039;csc&#039;)\n \n    rw_norm_adj = deg_inv.dot(adj)  # D_out^-1 * A\nThis ensures total freight leaving each location sums to 1.\nDirectional information is preserved.\nFor the Laplacian version:\nif gso_type == &#039;rw_norm_lap_out&#039;:\n    gso = id - rw_norm_adj\nelse:\n    gso = rw_norm_adj\nwhich follows:\nL_{\\text{rw-out}} = I - D_{\\text{out}}^{-1} A\n\n8. Summary of Key Design Choices\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nChoiceReason❌ No symmetrizationFreight flow is directional.❌ No self-loopsFreight does not return to its source.✅ Uses random walk normalizationEnsures each node’s outgoing flows sum to 1.✅ Supports Laplacian versionsUseful for diffusion-based models.\nFor freight flow modeling, the best GSO type is:\nrw_norm_adj_out\nor for a diffusion approach:\nrw_norm_lap_out"},"Standardized-Input":{"slug":"Standardized-Input","filePath":"Standardized Input.md","title":"Standardized Input","links":[],"tags":[],"content":"example input\nNode features:\nTime | Node 1 feature | Node 2 feature | ... | Node N feature\n------------------------------------------------------------\nt1   | x11            | x12            | ... | x1N\nt2   | x21            | x22            | ... | x2N\n...\ntT   | xT1            | xT2            | ... | xTN\n\nEdge features (one set per edge):\nEdge | From Node | To Node | Capacity | Speed limit | Lanes\n-----------------------------------------------------------\ne1   | 1       | 2        | 1000     | 80 km/h    | 3\ne2   | 2       | 3        | 1500     | 60 km/h    | 2\n\n• Node features: Typically dynamic data (e.g., node-level temporal values such as freight volume, delays, etc.).\n• Edge features: Typically static, used to build GSOs.\n• If edges have temporal changes, you’d have separate adjacency matrices per time step, or store edge features separately and integrate them in GNN message passing explicitly (more advanced approach)."},"_old/STGCN_original/1_Modules/2_data_loader/data.utils.py":{"slug":"_old/STGCN_original/1_Modules/2_data_loader/data.utils.py","filePath":"_old/STGCN_original/1_Modules/2_data_loader/data.utils.py.md","title":"data.utils.py","links":[],"tags":[],"content":"\ndata_gen()"},"_old/STGCN_original/1_Modules/2_data_loader/data_loader":{"slug":"_old/STGCN_original/1_Modules/2_data_loader/data_loader","filePath":"_old/STGCN_original/1_Modules/2_data_loader/data_loader.md","title":"data_loader","links":["graph-structure","nodes"],"tags":[],"content":"load_adj()\nThe load_adj() function is responsible for loading the [[adjacency matrix]] of the graph structure from a stored .npz file. This adjacency matrix represents the connectivity between nodes and is a crucial input for the Spatio-Temporal Graph Convolutional Network.\n1. Loading the Adjacency Matrix\n    dataset_path = &#039;./data&#039;\n    dataset_path = os.path.join(dataset_path, dataset_name)\n    adj = sp.load_npz(os.path.join(dataset_path, &#039;adj.npz&#039;))\n\nThe adjacency matrix file (adj.npz) is stored inside the dataset folder.\nsp.load_npz() loads the adjacency matrix, which is stored in Compressed Sparse Row (CSR) format.\nThe file is retrieved from ./data/{dataset_name}/adj.npz, allowing flexibility for different datasets.\n\n2. Converting the Adjacency Matrix to CSC Format\n    adj = adj.tocsc()\n\nConverts the adjacency matrix from CSR (Compressed Sparse Row) format to CSC (Compressed Sparse Column) format.\nWhy use CSC?\n\nCSC format is often faster for column-based access, making it useful for certain types of graph computations.\nIt improves memory efficiency when performing matrix-vector multiplications, which are common in graph neural networks.\n\n\n\n3. Defining the Number of Vertices\n    n_vertex = &lt;user-defined&gt;\n\nThe function returns n_vertex, which represents the number of nodes in the graph.\nThis should be manually specified for each dataset to ensure proper alignment with the adjacency matrix.\nWhen integrating a custom dataset, replace the predefined values with the correct number of nodes.\n\n4. Returning the Adjacency Matrix and Node Count\n    return adj, n_vertex\n\nThe function outputs:\n\nadj → The adjacency matrix in CSC sparse format.\nn_vertex → The total number of nodes in the dataset.\n\n\n"},"_old/STGCN_original/1_Modules/2_models/models":{"slug":"_old/STGCN_original/1_Modules/2_models/models","filePath":"_old/STGCN_original/1_Modules/2_models/models.md","title":"models","links":[],"tags":[],"content":""},"_old/STGCN_original/1_Modules/2_models/tester.py":{"slug":"_old/STGCN_original/1_Modules/2_models/tester.py","filePath":"_old/STGCN_original/1_Modules/2_models/tester.py.md","title":"tester.py","links":[],"tags":[],"content":"model_test()"},"_old/STGCN_original/1_Modules/2_models/trainer.py":{"slug":"_old/STGCN_original/1_Modules/2_models/trainer.py","filePath":"_old/STGCN_original/1_Modules/2_models/trainer.py.md","title":"trainer.py","links":[],"tags":[],"content":"model_train()"},"_old/STGCN_original/1_Modules/2_utils/math_graph.py":{"slug":"_old/STGCN_original/1_Modules/2_utils/math_graph.py","filePath":"_old/STGCN_original/1_Modules/2_utils/math_graph.py.md","title":"math_graph.py","links":[],"tags":[],"content":"scaled_laplacian()\ncheb_poly_approx()\nweight_matrix()\nscaled_laplacian()"},"_old/STGCN_original/1_Modules/2_utils/utils":{"slug":"_old/STGCN_original/1_Modules/2_utils/utils","filePath":"_old/STGCN_original/1_Modules/2_utils/utils.md","title":"utils","links":[],"tags":[],"content":""},"_old/STGCN_original/Old-notes-for-STGCN":{"slug":"_old/STGCN_original/Old-notes-for-STGCN","filePath":"_old/STGCN_original/Old notes for STGCN.md","title":"Old notes for STGCN","links":["_old/STGCN_original/1_Modules/2_utils/utils","Graph-theory/GCN-Kernel","_old/STGCN_original/1_Modules/2_data_loader/data_loader","_old/STGCN_original/1_Modules/2_models/models"],"tags":[],"content":"Weighted Adjacency Matrix and Graph Kernel\nThe next part of the code loads the weighted adjacency matrix into the memory.  It uses function [[math_graph.py#|#weight_matrix()]] in utils module.\nSubsequently, [[math_graph.py#|#scaled_laplacian()]] function is used to calculate a normalized Laplacian graph kernel.\n\nGraph Kernel - is a matrix transformation that defines how information propagates through a graph. In this case, the scaled Laplacian acts as a graph kernel. It takes the weighted adjacency matrix (which shows which nodes are connected and how strong their relationship is) and transforms it into a new representation that captures the structure of the graph more effectively.\n\nThe resulting Laplacian graph kernel is passed as an argument, together with kernel size and number of routes, to [[math_graph.py#|#cheb_poly_approx()]] which uses Chebyshev polynomial approximation to approximate the graph convolution kernel.\nGCN Kernel\nData processing\nAt this point the actual data gets loaded and split into training, validation and test sets.\nThe original values are:\n\nn_train = 34,\nn_val = 5,\nn_test = 5\nA possible explanation for choosing these number is efficiency of the training and increasing the computational efficiency due to faster convergence.\n\nThe next step is to define the data and its parameters for network training. To achieve this [[data.utils.py#|#data_gen()]] from data_loader module is used. It requires the following parameters:\n\nfile_path: str, the file path of data source.\ndata_config: tuple, the configs of dataset in train, validation, test.\nn_route: int, the number of routes in the graph.\nn_frame: int, the number of frame within a standard sequence unit,\nwhich contains n_his = 12 and n_pred = 9 (3 /15 min, 6 /30 min &amp; 9 /45 min).\nday_slot: int, the number of time slots per day, controlled by the time window (5 min as default).\n\nIt returns a [[data.utils.py#|#Dataset]] class object (found in data_loader module).\nModel Training and Testing\nIn this section two functions are called:\n\n[[trainer.py#|#model_train()]] from models module\n[[tester.py#|#model_test()]] from models\n"},"_old/STGCN_original/TensorFlow-+-Metal":{"slug":"_old/STGCN_original/TensorFlow-+-Metal","filePath":"_old/STGCN_original/TensorFlow + Metal.md","title":"TensorFlow + Metal","links":[],"tags":[],"content":"For apple I used TensorFlow + Metal\nRequirements:\n\nMac computers with Apple silicon or AMD GPUs\nmacOS 12.0 or later (Get the latest beta)\nPython 3.9 or later\nXcode command-line tools: xcode-select --install\n\nFor TensorFlow version 2.13 or later:\npython -m pip install tensorflow\n\nand to install tensorflow-metal plugin:\npython -m pip install tensorflow-metal\n"},"index":{"slug":"index","filePath":"index.md","title":"index","links":["main.py","Edges","Vertices","Graph-theory/adjacency-matrix","Feature-matrix","Graph-theory/Graph-Shift-Operators"],"tags":[],"content":"Scripts\nmain.py\nGraph Theory\nEdges\nVertices\nAdjacency matrices\nfeature matrices\nGraph Shift Operators"},"main.py":{"slug":"main.py","filePath":"main.py.md","title":"main.py","links":["tags/python","tags/ST-SimNet","_old/STGCN_original/TensorFlow-+-Metal","STGCN_hazdzz/script/script","STGCN_hazdzz/model/model","Graph-theory/Bottleneck-Design","Graph-theory/adjacency-matrix","Vertices","Graph-Shift-Operator","literature/GLU_Language-Modeling-with-Gated-Convolutional-Networks.pdf","literature/ChebNet_Convolutional-Neural-Networks-on-Graphs-with-Fast-Localized-Spectral-Filtering.pdf","literature/How-Does-Information-Bottleneck-Help-Deep-Learning.pdf"],"tags":["python","ST-SimNet"],"content":"python ST-SimNet\nInstallation and requirements\nThe code was supposed to be based on [pytorch][pytorch.org/] and [torch_geometric_temporal][github.com/benedekrozemberczki/pytorch_geometric_temporal/blob/master/torch_geometric_temporal/nn/attention/stgcn.py], but apparently is built on tensor flow. Considering the age of the code, many dependencies are not supported or built only for linux/windows/apple intel. I use apple silicon M1. I had to decide whether to recreate the environment - I tried with pyenv, miniforge, conda, but there are too many incompatibilites, so to keep the code robust and useful in the future, I decided to rewrite it to pytorch. Fortunately, while reading github issues in the [main repository of STGCN][github.com/VeritasYin/STGCN_IJCAI-18] I found an [improved version of STGCN in with torch][github.com/hazdzz/STGCN/tree/main], which I decided to adapt for my project due to more approachable character of torch and time constraints.\nRunning main.py\nSetting GPU\nIn the original version of STGCN, I added GPU support to avoid memory error - using PluggableDevice for TensorFlow. Also, I installed TensorFlow + Metal for Macbooks.\nIn the torch version of STGCN GPU support has already been added and enabled as default.\nStructure\nTorch STGCN\nmain.py imports 2 modules:\n\nscript\nmodel\n\n\nEnsuring Reproducibility in ST-SimNet\nTo maintain consistent results across different runs, it is essential to set random seeds and enforce deterministic behavior in PyTorch. This is handled in the set_env(seed) function, which configures environment variables and library-specific seeds to ensure reproducibility.\nExplanation of set_env(seed)\nThis function ensures that randomness is controlled across Python, NumPy, and PyTorch, minimizing variability in machine learning experiments. Below is a breakdown of its key components:\n\n\nSet Python Hash Seed\nos.environ[&#039;PYTHONHASHSEED&#039;] = str(seed)\n\nEnsures that Python’s built-in hashing functions behave consistently across runs.\n\n\n\nSet Random Seeds for Libraries\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nControls randomness for Python’s random module, NumPy, and PyTorch (both CPU and GPU).\n\n\n\nEnsure Deterministic CUDA Behavior\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = True\n\nPrevents CuDNN (NVIDIA’s deep learning library) from selecting non-deterministic algorithms.\nThis enforces reproducibility but may slightly impact performance.\n\n\n\nCommented-Out Sections\nSeveral environment variables are commented out in the function:\n# os.environ[&#039;CUDA_VISIBLE_DEVICES&#039;] = &#039;0, 1&#039;\n# os.environ[&#039;CUDA_LAUNCH_BLOCKING&#039;] = &#039;1&#039;\n# os.environ[&#039;CUBLAS_WORKSPACE_CONFIG&#039;] = &#039;:16:8&#039;\n# os.environ[&#039;CUBLAS_WORKSPACE_CONFIG&#039;] = &#039;:4096:8&#039;\n# torch.use_deterministic_algorithms(True)\n\nCUDA_VISIBLE_DEVICES: Restricts PyTorch to specific GPUs.\nCUDA_LAUNCH_BLOCKING: Forces CUDA operations to execute synchronously (useful for debugging but slows execution).\nCUBLAS_WORKSPACE_CONFIG: Ensures deterministic matrix operations for CUDA-based computations.\ntorch.use_deterministic_algorithms(True): Enforces strict determinism but may lead to errors if certain operations lack deterministic implementations.\n\nThese settings are commented out to allow flexibility, avoid unnecessary debugging constraints, or prevent performance degradation.\nPractical Implications\nBy including set_env(seed), ST-SimNet ensures that experiments remain consistent and reproducible. However, enforcing determinism can come at a computational cost, particularly when using GPU acceleration. If exact reproducibility is required (e.g., for academic research or debugging), enabling deterministic settings is recommended. Otherwise, for general usage, some flexibility in randomness may be acceptable to optimize performance.\n\nParsing Arguments in STGCN\nget_parameters()\nThe function initializes an ArgumentParser instance and defines the available command-line arguments. When executed, it reads the user-provided arguments (or uses defaults if none are given) and returns them as a structured object (args). This allows flexible customization of the model’s behavior without modifying the script itself.\nBy configuring these parameters, users can fine-tune the STGCN model for different datasets, optimize hyperparameters, and enable/disable various features efficiently.\nOverview of Parameters\nBelow is a breakdown of the command-line arguments and their roles in the model:\n\n\nGeneral Settings\n\n--enable_cuda (bool, default=True): Enables or disables CUDA for GPU acceleration.\n--seed (int, default=42): Sets the random seed for reproducibility.\n--dataset (str, default=‘metr-la’): Specifies the dataset to be used.\n\n\n\nModel Input Parameters\n\n--n_his (int, default=12): Number of historical time steps used as input.\n--n_pred (int, default=3): Number of future time steps to predict.\n--time_intvl (int, default=5): Defines the time interval between observations.\n\n\n\nModel Architecture Settings\n\n--Kt (int, default=3): Kernel size for temporal convolution.\n--stblock_num (int, default=2): Number of Spatio-Temporal Convolution Blocks (STConvBlocks).\n--act_func (str, default=‘glu’): Activation function for the model (glu or gtu)1.\n--Ks (int, default=3): Kernel size for spatial convolution.\n--graph_conv_type (str, default=‘cheb_graph_conv’): Type of graph convolution (cheb_graph_conv or graph_conv)2.\n--gso_type (str, default=‘sym_norm_lap’): Graph structure operator type. Choices include sym_norm_lap, rw_norm_lap, sym_renorm_adj, and rw_renorm_adj.\n--enable_bias (bool, default=True): Whether to enable bias in the model.\n--droprate (float, default=0.5): Dropout rate for regularization.\n\n\n\nTraining Hyperparameters\n\n--lr (float, default=0.001): Learning rate for training.\n--weight_decay_rate (float, default=0.001): Weight decay (L2 regularization) for optimizer.\n--batch_size (int, default=32): Batch size used for training.\n--epochs (int, default=1000): Number of training epochs.\n--opt (str, default=‘adamw’): Optimizer choice (adamw, nadamw, lion).\n--step_size (int, default=10): Step size for learning rate scheduler.\n--gamma (float, default=0.95): Decay factor for learning rate.\n--patience (int, default=10): Number of epochs to wait before early stopping is triggered.\n\n\n\nInitializing the Training Environment and Model Configuration\n1. Ensuring Stable Experiment Results\nset_env(args.seed)\n\nCalls the set_env function to set random seeds across Python, NumPy, and PyTorch.\nEnables deterministic behavior in CUDA to ensure reproducibility between runs.\n\n2. Selecting the Computing Device\nif args.enable_cuda and torch.cuda.is_available():\n    device = torch.device(&#039;cuda&#039;)\n    torch.cuda.empty_cache()  # Clean cache\nelse:\n    device = torch.device(&#039;cpu&#039;)\n    gc.collect()  # Clean cache\n\nIf CUDA is enabled and available, the model runs on the GPU (cuda).\nIf no GPU is detected, it defaults to the CPU (cpu).\nCalls torch.cuda.empty_cache() to free GPU memory.\nCalls gc.collect() to optimize CPU memory usage.\n\n3. Computing Ko (Temporal Reduction Factor)\nKo = args.n_his - (args.Kt - 1) * 2 * args.stblock_num\n\n\nDetermines the remaining temporal dimension after applying Spatio-Temporal Convolutional Blocks (STConvBlocks).\n\n\nFormula Breakdown:\n\nargs.n_his → Number of historical time steps.\n(args.Kt - 1) * 2 * args.stblock_num → The reduction due to multiple temporal convolutions.\n\n\n\nThis ensures that after multiple STConvBlocks, the output size is correct for the final layers.\n\n\nEnsures that after multiple STConvBlocks, the output size is correct for the final layers.\n\n\nWhen is Ko equal to 0?\nKo can be 0 when the number of historical time steps is fully consumed by the STConvBlocks’ receptive fields, meaning there are no additional temporal steps left for further layers. This typically happens when args.n_his is small relative to the number of blocks and temporal kernel size.\n\n\nWhen is Ko greater than 0?\nKo is greater than 0 when there are remaining temporal steps available after passing through the convolutional layers. This allows additional fully connected layers to process the output, enhancing feature representation before the final prediction.\n\n\n4. Defining the Model Architecture\nBottleneck Design\nSTGCN consists of two Spatio-Temporal Convolution Blocks 3\nThe bottleneck strategy is a common deep learning technique used to optimize network architectures by reducing computational costs while maintaining or improving model performance. It achieves this by:\n\nReducing the number of channels in intermediate layers - to lower computational complexity\nProcessing the reduced representation using convolutions\nExpanding the number of channels again - to retain useful information\n\nblocks = []\nblocks.append([1])  # Initial input channel\nfor l in range(args.stblock_num):\n    blocks.append([64, 16, 64])\n\nblocks defines the layer structure of the STGCN model.\nUses a bottleneck strategy (64 → 16 → 64) to reduce computation while retaining information.\n\nif Ko == 0:\n    blocks.append([128])\nelif Ko &gt; 0:\n    blocks.append([128, 128])\nblocks.append([1])  # Output layer\n\nIf Ko == 0, a single fully connected layer with 128 channels is added.\nIf Ko &gt; 0, an extra 128-channel layer is used before the final output layer.\nThe final [1] ensures the output shape is correct for predictions.\n\nMore on Bottleneck Design.\n5. Returning the Configuration\nreturn args, device, blocks\n\nReturns:\n\nargs → Parsed arguments containing user-defined configurations.\ndevice → The selected computing device (cuda or cpu).\nblocks → The defined neural network architecture based on user parameters.\n\n\n\nSummary\n\nEnsures reproducibility by setting random seeds (set_env).\nSelects between GPU or CPU for training.\nComputes Ko, which determines the size of the temporal output.\nConstructs a bottleneck-based architecture for efficient computation.\nReturns the necessary configurations for training the STGCN model.\n\n\nData Preparation in STGCN\nThe data_preparate() function is responsible for loading and processing the dataset before training the Spatio-Temporal Graph Convolutional Network. It performs several key tasks, including loading the adjacency matrix, computing the graph shift operator, applying transformations, and creating data loaders for training, validation, and testing.\n1. Loading the Adjacency Matrix\nadj, n_vertex = dataloader.load_adj(args.dataset)\n\nLoads the adjacency matrix (adj) for the selected dataset (args.dataset) using [[dataloader#|#load_adj()]] function from script directory.\nn_vertex represents the number of nodes (Vertices) in the graph.\nThe adjacency matrix encodes the spatial structure of the graph, defining relationships between nodes.\n\n2. Calculating the Graph Shift Operator (GSO)\nGSO calculations can be found [[utility#|#calc_gso()]] in  script module\ngso = utility.calc_gso(adj, args.gso_type)\n\nThe **Graph Shift Operator is computed from the adjacency matrix.\nThe GSO defines how information propagates through the graph.\nargs.gso_type specifies the type of graph operator used (e.g., normalized Laplacian, random walk Laplacian).\n\n3. Applying Chebyshev Approximation (If Needed)\nif args.graph_conv_type == &#039;cheb_graph_conv&#039;:\n    gso = utility.calc_chebynet_gso(gso)\n\nIf Chebyshev graph convolution (cheb_graph_conv) is selected, the function computes the Chebyshev approximation.\nThis approximation reduces the complexity of the Graph Convolutional Network (GCN) while maintaining expressiveness.\n\n4. Converting GSO to a Tensor\ngso = gso.toarray()\ngso = gso.astype(dtype=np.float32)\nargs.gso = torch.from_numpy(gso).to(device)\n\nConverts the graph shift operator (GSO) to a NumPy array and ensures it is in float32 format.\nConverts it into a PyTorch tensor (torch.from_numpy) and transfers it to the specified device (CPU or GPU).\nThis step ensures that the model can efficiently perform graph-based operations during training.\n\n\nRemarks\nProbably I need to adjust also parsed arguments\nAdd something about basic working of GNNs\nTo-do\n\n standardize the input\n modify the input to accept:\n\n edges\n nodes\n\n\n\nFootnotes:\nFootnotes\n\n\nThe concept of GLU was first introduced in the paper GLU_Language Modeling with Gated Convolutional Networks.pdf ↩\n\n\nChebNet_Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering.pdf ↩\n\n\nHow Does Information Bottleneck Help Deep Learning.pdf ↩\n\n\n"}}